# LOTVS-Cognitive Accident Prediction
We are wormaly to release a new benchmark on Accident Prediction in dashcam videos. The benchmark is called as CAP-DATA, which consists of 11,727 videos with 2.19 million frames. The fact- effect-reason-introspection description for each accident video is annotated, which consists of the factual description before accident, categorical accident description, accident reason description and preventive advice description. To our best knowledge, it is the largest accident prediction benchmark in driving scenarios.

In addition, we also propose a new model to fulfill a Cognitive Accident Prediction (CAP), which explores the human cognition clues to assist accident prediction in driving videos. Particularly, we explore the text description before accident and the driver attention in driving situations to boost the explainability of accident prediction model, which has the apparent semantic guidance for the object to be involved in accident and helps to find the crashing object efficiently. Extensive experiments shows that the proposed method can obtain larger Time-to-Accident (TTA) than other state-of-the-arts.


![image](https://github.com/JWFanggit/LOTVS-CAP/blob/main/model.png)
# Testing
Min-Test(DADA-2000-TEST):Download the best model [Here](https://pan.baidu.com/s/1tgXcaEaWQdgmoB7eubuZfA)(Extraction code:i8mg)

Full-Test(CAP-DATA-TEST):Download the best model [Here](https://pan.baidu.com/s/13iFDdi_aInqQBFOJHOXl8w)(Extraction code:keh4)

# Dataset download:
We are worm-hearted to release this benchmark here, and sincerely invite to use and share it. Our CAP-DATA can be downloaded from （）This is the dataset that we re-uploaded after sorting out. At the same time, the complete DADA-2000 dataset can be download from（）.The DADA-Small dataset you can follow  the [work](https://github.com/Cogito2012/DRIVE.git)@inproceedings{BaoICCV2021DRIVE, author = "Bao, Wentao and Yu, Qi and Kong, Yu", title = "Deep Reinforced Accident Anticipation with Visual Explanation",booktitle = "International Conference on Computer Vision (ICCV)",year = "2021"}

Note: CAP-DATA benchmark can only be utilized for research. If you are interested in this work and the benchmark, please cite the work with following bibtex.
