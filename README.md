# LOTVS-CAP
The proposed CAP model consists of three core modules:attentive text-to-vision shift fusion module, attentive semantic context transfer module, and driver attention guided accident prediction module. It is worth noting that each module explores the attention mechanism and contributes a brain inspired cognitive modeling. We learn the coherence of text-vision semantics and the activated scene context by cascade attentive networks, where each attentive module fulfills the core semantics learning for accident prediction. Attentive text to-vision shift fusion module is modeled by inferring the coherently semantic relation of text and video for accident prediction. Besides, text-to-video shift strategy aims to leverage the semantic knowledge in text to video, so as to adapt to the testing phase only with the video data in practical use. Attentive semantic context transfer module encodes the scene context that is modeled by the Graph Neural Network (GNN) and Gated Recurrent Unit (GRU),
which imitates the ability of human-beings for historical and contextual memory learning. Then, the driver attention guided accident prediction module predicts the beginning time of road collision and reconstructs the driver attention map simultaneously. Exhaustive experiments shows that the proposed method can obtain larger Time-to-Accident (TTA) than other state-of-the-arts.
